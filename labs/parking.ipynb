{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from shutil import copy2 as copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parking Annot to TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('amano-nvr-parking.json') as f:\n",
    "    context = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest = Path('/home/jiun/datasets/amano/raw/parking')\n",
    "dest.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for info in context:\n",
    "    name, *_ = info['name'].split('.')\n",
    "    boxes = np.array([box['position'] for box in info['bounding_boxes']])\n",
    "    pd.DataFrame(boxes).to_csv(str(dest.joinpath(f'{name}.txt')), header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jiun/workspace/Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = 'efficientnet-b0'\n",
    "weight = '/home/jiun/datasets/weights/grid/eff0-128.pth'\n",
    "shape = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "model = Model.new(backbone, class_num=2, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(weight, map_location=lambda s, l: s))\n",
    "model.eval()\n",
    "print('Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "def show(ary):\n",
    "    display(Image.fromarray(ary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_src = Path('/home/jiun/datasets/amano/raw/parking')\n",
    "parkings = sorted(parking_src.glob('*.txt'))\n",
    "images_src = Path('/home/jiun/datasets/amano/nvr-warp/test/images')\n",
    "annots_src = Path('/home/jiun/datasets/amano/nvr-warp/test/annotations')\n",
    "dest = Path('/home/jiun/datasets/amano/nvr-warp/test/parking-eff0-128')\n",
    "dest.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = (.485, .456, .406), (.229, .224, .225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/256 [00:00<?, ?it/s]/home/jiun/conda/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 256/256 [05:20<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "for parking_file in tqdm(parkings):\n",
    "    try:\n",
    "        parking = pd.read_csv(str(parking_file), header=None).values\n",
    "    except:\n",
    "        parking = np.empty((0, 4), dtype=np.int16)\n",
    "    \n",
    "    images = sorted(images_src.glob(f'{parking_file.stem}*.jpg'))\n",
    "    annots = sorted(annots_src.glob(f'{parking_file.stem}*.txt'))\n",
    "    \n",
    "    for image, annot in zip(images, annots):\n",
    "        try:\n",
    "            g = pd.read_csv(str(annot), header=None).values\n",
    "        except:\n",
    "            g = np.empty((0, 4), dtype=np.int16)\n",
    "        \n",
    "        img = cv2.imread(str(image))\n",
    "        \n",
    "        results = np.empty((0, 2), dtype=np.float)\n",
    "        for x, y, w, h in parking:\n",
    "            inputs = img[y:y+h, x:x+w]\n",
    "            \n",
    "            if y > 540:\n",
    "                inputs = cv2.flip(inputs, 0)\n",
    "            \n",
    "            inputs = transform(inputs).to(device)\n",
    "            inputs = Variable(inputs.unsqueeze(0), requires_grad=False)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            conf, pred = torch.max(F.softmax(outputs), 1)\n",
    "            \n",
    "            results = np.concatenate((\n",
    "                results,\n",
    "                np.array([(pred.item(), conf.item())]),\n",
    "            ))\n",
    "            \n",
    "        pd.DataFrame(results).to_csv(str(dest.joinpath(annot.name)), header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest = Path('/home/jiun/datasets/amano/nvr-warp/test/parking-eff0-128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted(images_src.glob('*.jpg'))\n",
    "annots = sorted(annots_src.glob('*.txt'))\n",
    "parkings = sorted(dest.glob('*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(box, pos, ratio=.0):\n",
    "    x1, y1, x2, y2 = box\n",
    "    xx, yy = pos\n",
    "    w, h = x2-x1, y2-y1\n",
    "    return x1-w*ratio <= xx <= x2+w*ratio and y1-h*ratio <= yy <= y2+h*ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(n_class=2, method=Evaluator.compute_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1313/1313 [01:11<00:00, 18.46it/s]\n"
     ]
    }
   ],
   "source": [
    "for image, annot, parking in tqdm(zip(images, annots, parkings), total=len(images)):\n",
    "    img = cv2.imread(str(image))\n",
    "    flag, *_ = image.stem.split('_CT_')\n",
    "    \n",
    "    try:\n",
    "        g = pd.read_csv(str(annot), header=None).values\n",
    "    except:\n",
    "        g = np.empty((0, 4), dtype=np.int16)\n",
    "    \n",
    "    try:\n",
    "        pp = pd.read_csv(str(parking_src.joinpath(f'{flag}.txt')), header=None).values\n",
    "        pp[:, 2] += pp[:, 0]\n",
    "        pp[:, 3] += pp[:, 1]\n",
    "        p = pd.read_csv(str(parking), header=None).values\n",
    "    except:\n",
    "        pp = np.empty((0, 4), dtype=np.int16)\n",
    "        p = np.empty((0, 4), dtype=np.int16)\n",
    "\n",
    "    assert len(pp) == len(p)\n",
    "    \n",
    "    for x, y, w, h in g:\n",
    "        cv2.rectangle(img, (int(x), int(y)), (int(w), int(h)), (0, 0, 255), 1)\n",
    "    \n",
    "    for klass, (x, y, w, h) in zip(p[:, 0], pp):\n",
    "        cv2.rectangle(img, (int(x), int(y)), (int(w), int(h)), (0, 255, 0) if klass else (255, 0, 0), 1)\n",
    "        box = np.array((x, y, x+w, y+h))\n",
    "    \n",
    "    # extract gt only parking lot defined\n",
    "    p_index = []\n",
    "    g_index = []\n",
    "    for i, box in enumerate(pp):\n",
    "        flag = False\n",
    "        for j, pos in enumerate(zip((g[:, 0] + g[:, 2]) / 2, (g[:, 1] + g[:,3]) / 2)):\n",
    "            if check(box, pos):\n",
    "                p_index.append(i)\n",
    "                g_index.append(j)\n",
    "                flag = True\n",
    "                break\n",
    "    p_index = list(set(p_index))\n",
    "    g_index = list(set(g_index))\n",
    "    \n",
    "    # extract only detection (classifier)\n",
    "    det = pp[np.array(p_index, dtype=np.int)[p[:, 0][p_index].astype(np.bool)]]\n",
    "    get = g[g_index]\n",
    "    \n",
    "#     for x, y, w, h in get:\n",
    "#         cv2.rectangle(img, (int(x), int(y)), (int(w), int(h)), (0, 0, 255), 1)\n",
    "    \n",
    "#     for x, y, w, h in det:\n",
    "#         cv2.rectangle(img, (int(x), int(y)), (int(w), int(h)), (0, 255, 0), 1)\n",
    "    \n",
    "#     show(img)\n",
    "    \n",
    "    evaluator.update((\n",
    "        np.ones(np.size(det, 0), dtype=np.int),\n",
    "        np.ones(np.size(det, 0), dtype=np.float32),\n",
    "        det.astype(np.float32),\n",
    "        None,\n",
    "    ), (\n",
    "        np.ones(np.size(get, 0), dtype=np.int),\n",
    "        get.astype(np.float32),\n",
    "        None,\n",
    "    ))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiun/conda/lib/python3.7/site-packages/ipykernel_launcher.py:126: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/jiun/conda/lib/python3.7/site-packages/ipykernel_launcher.py:131: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.98398284, 0.98398284])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiun/conda/lib/python3.7/site-packages/ipykernel_launcher.py:126: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/jiun/conda/lib/python3.7/site-packages/ipykernel_launcher.py:131: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.98398284, 0.98398284]),\n",
       " array([0.        , 0.99947964]),\n",
       " array([0.        , 0.98449513]))"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDetection format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_src = Path('/home/jiun/datasets/amano/raw/parking')\n",
    "parkings = sorted(parking_src.glob('*.txt'))\n",
    "images_src = Path('/home/jiun/datasets/amano/nvr-warp/test/images')\n",
    "annots_src = Path('/home/jiun/datasets/amano/nvr-warp/test/annotations')\n",
    "detection_src = Path('/home/jiun/datasets/amano/nvr-warp/results/ssd-vgg16')\n",
    "dest = Path('/home/jiun/datasets/amano/nvr-warp/test/parking-eff0-128')\n",
    "dest.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parking_file in tqdm(parkings):\n",
    "    try:\n",
    "        parking = pd.read_csv(str(parking_file), header=None).values\n",
    "    except:\n",
    "        parking = np.empty((0, 4), dtype=np.int16)\n",
    "    \n",
    "    images = sorted(images_src.glob(f'{parking_file.stem}*.jpg'))\n",
    "    annots = sorted(annots_src.glob(f'{parking_file.stem}*.txt'))\n",
    "    \n",
    "    for image, annot in zip(images, annots):\n",
    "        try:\n",
    "            g = pd.read_csv(str(annot), header=None).values\n",
    "        except:\n",
    "            g = np.empty((0, 4), dtype=np.int16)\n",
    "        \n",
    "        img = cv2.imread(str(image))\n",
    "        \n",
    "        results = np.empty(0, dtype=np.uint8)\n",
    "        for x, y, w, h in parking:\n",
    "            inputs = img[y:y+h, x:x+w]\n",
    "            \n",
    "            if y > 540:\n",
    "                inputs = cv2.flip(inputs, 0)\n",
    "            \n",
    "            inputs = transform(inputs).to(device)\n",
    "            inputs = Variable(inputs.unsqueeze(0), requires_grad=False)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            results = np.append(results, preds.item())\n",
    "        pd.DataFrame(results).to_csv(str(dest.joinpath(annot.name)), header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_iou_(a, b):\n",
    "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "\n",
    "    iw = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 0])\n",
    "    ih = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 1])\n",
    "\n",
    "    iw = torch.clamp(iw, min=0)\n",
    "    ih = torch.clamp(ih, min=0)\n",
    "\n",
    "    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih\n",
    "\n",
    "    ua = torch.clamp(ua, min=1e-8)\n",
    "\n",
    "    intersection = iw * ih\n",
    "\n",
    "    IoU = intersection / ua\n",
    "\n",
    "    return IoU\n",
    "\n",
    "\n",
    "def compute_iou(box, boxes, box_area, boxes_area):\n",
    "    y1 = np.maximum(box[0], boxes[:, 0])\n",
    "    y2 = np.minimum(box[2], boxes[:, 2])\n",
    "    x1 = np.maximum(box[1], boxes[:, 1])\n",
    "    x2 = np.minimum(box[3], boxes[:, 3])\n",
    "    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
    "    union = box_area + boxes_area[:] - intersection[:]\n",
    "    iou = intersection / union\n",
    "    return iou\n",
    "\n",
    "\n",
    "def compute_overlaps(boxes1, boxes2):\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "\n",
    "    overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))\n",
    "    for i in range(overlaps.shape[1]):\n",
    "        overlaps[:, i] = compute_iou(boxes2[i], boxes1, area2[i], area1)\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, n_class: int, sample_patch: int = 11, threshold: float = .5, method=None):\n",
    "        self.n_class = n_class\n",
    "        self.patch = np.linspace(0, 1, sample_patch)\n",
    "        self.threshold = threshold\n",
    "        self.method = method or self.compute_iou\n",
    "\n",
    "        self.center_total = np.empty((0, 2), dtype=np.int)\n",
    "        self.center_positive = np.empty((0, 2), dtype=np.int)\n",
    "\n",
    "        self.TP, self.FP, self.FN = np.zeros((3, sample_patch, n_class), dtype=np.uint32)\n",
    "        self.gt_counts, self.pd_counts = np.zeros((2, n_class), dtype=np.uint32)\n",
    "\n",
    "    def update(self, predictions: Tuple[np.ndarray, Union[np.ndarray, None], np.ndarray, Union[np.ndarray, None]],\n",
    "               groundtruths: Tuple[np.ndarray, np.ndarray, Union[np.ndarray, None]]) \\\n",
    "            -> None:\n",
    "        \"\"\"Update predictions and groundtruths for each frames.\n",
    "\n",
    "        :param predictions: contains (class_ids, scores, bounding_boxes, masks)\n",
    "        :param groundtruths: contains (class_ids, bounding_boxes, masks)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pd_class_ids, pd_scores, pd_bboxes, pd_masks = predictions\n",
    "        gt_class_ids, gt_bboxes, gt_masks = groundtruths\n",
    "\n",
    "        if pd_scores is None:\n",
    "            pd_scores = np.ones_like(pd_class_ids)\n",
    "\n",
    "        if pd_bboxes.size > 0:\n",
    "            iou = self.method(pd_bboxes, gt_bboxes, self.threshold)\n",
    "\n",
    "        for klass in range(self.n_class):\n",
    "            gt_number = np.sum(gt_class_ids == klass)\n",
    "\n",
    "            self.gt_counts[klass] += gt_number\n",
    "            self.pd_counts[klass] += (pd_class_ids == klass).sum()\n",
    "\n",
    "            for p, patch in enumerate(self.patch):\n",
    "                pd_mask = np.logical_and(pd_class_ids == klass, pd_scores >= patch)\n",
    "                pd_number = np.sum(pd_mask)\n",
    "\n",
    "                if pd_number == 0:\n",
    "                    self.FN[p][klass] += gt_number\n",
    "                    continue\n",
    "\n",
    "                # X, Y distribution store\n",
    "                centers = np.stack((pd_bboxes[:, ::2].mean(-1), pd_bboxes[:, 1::2].mean(-1))).T.astype(np.int)\n",
    "                self.center_total = np.concatenate((self.center_total, centers))\n",
    "                self.center_positive = np.concatenate((self.center_positive, centers[pd_mask]))\n",
    "\n",
    "                true_positive = np.sum(iou[pd_mask][:, gt_class_ids == klass].any(axis=0))\n",
    "\n",
    "                self.TP[p][klass] += true_positive\n",
    "                self.FP[p][klass] += pd_number - true_positive\n",
    "                self.FN[p][klass] += gt_number - true_positive\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_acc(tar_boxes: np.ndarray, src_boxes: np.ndarray, threshold: float) \\\n",
    "            -> np.ndarray:\n",
    "        results = np.empty((0, np.size(src_boxes, 0)), dtype=np.bool)\n",
    "        xxs, yys = (src_boxes[:, 0] + src_boxes[:, 2])/2, (src_boxes[:, 1] + src_boxes[:, 3])/2\n",
    "        for tar in tar_boxes:\n",
    "            results = np.concatenate((results, np.expand_dims(np.array(tuple(check(tar, (x, y)) for x, y in zip(xxs, yys))), 0)))\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_iou(tar_boxes: np.ndarray, src_boxes: np.ndarray, threshold: float) \\\n",
    "            -> np.ndarray:\n",
    "        iou = compute_overlaps(tar_boxes, src_boxes) >= threshold\n",
    "        axis, argm = np.arange(np.size(iou, 0)), iou.argmax(axis=1)\n",
    "        outputs = np.zeros_like(iou)\n",
    "        outputs[axis, argm] = iou[axis, argm]\n",
    "        return outputs >= threshold\n",
    "\n",
    "    @property\n",
    "    def precision(self) \\\n",
    "            -> np.ndarray:\n",
    "        return np.nan_to_num(self.TP / (self.TP + self.FP)).mean(axis=0)\n",
    "\n",
    "    @property\n",
    "    def recall(self) \\\n",
    "            -> np.ndarray:\n",
    "        return np.nan_to_num(self.TP / (self.TP + self.FN)).mean(axis=0)\n",
    "\n",
    "    @property\n",
    "    def mAP(self) \\\n",
    "            -> np.ndarray:\n",
    "        prev_recall, ap = np.zeros((2, self.n_class))\n",
    "\n",
    "        for precision, recall in zip(self.precision[::-1], self.recall[::-1]):\n",
    "            ap += precision * (recall - prev_recall)\n",
    "            prev_recall = recall\n",
    "\n",
    "        return ap\n",
    "\n",
    "    def dump(self) \\\n",
    "            -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        return self.mAP, self.precision, self.recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
